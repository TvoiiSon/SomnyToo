use std::sync::Arc;
use std::time::{Instant, Duration};
use tokio::sync::{RwLock, Mutex};
use tracing::info;
use dashmap::DashMap;

/// –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á–µ—Ä —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ ML
pub struct AdaptiveBatcher {
    config: AdaptiveBatcherConfig,
    current_batch_size: RwLock<usize>,
    metrics: RwLock<BatchMetrics>,
    window_metrics: Mutex<Vec<WindowMetric>>,
    metrics_store: Arc<DashMap<String, f64>>,

    // –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è ML-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    prediction_model: Mutex<PredictionModel>,
    workload_predictor: Mutex<WorkloadPredictor>,
    adaptation_history: Mutex<Vec<AdaptationRecord>>,
}

#[derive(Debug, Clone)]
pub struct AdaptiveBatcherConfig {
    pub min_batch_size: usize,
    pub max_batch_size: usize,
    pub initial_batch_size: usize,
    pub window_duration: Duration,
    pub target_latency: Duration,
    pub max_increase_rate: f64,
    pub min_decrease_rate: f64,
    pub adaptation_interval: Duration,
    pub enable_auto_tuning: bool,
    pub enable_predictive_adaptation: bool,
    pub prediction_horizon: Duration,
    pub smoothing_factor: f64,
    pub confidence_threshold: f64,
}

#[derive(Debug, Clone)]
pub struct BatchMetrics {
    pub total_batches: u64,
    pub total_items: u64,
    pub avg_batch_size: f64,
    pub avg_processing_time: Duration,
    pub p95_processing_time: Duration,
    pub p99_processing_time: Duration,
    pub last_adaptation: Instant,
    pub adaptation_count: u64,

    // –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è ML
    pub prediction_accuracy: f64,
    pub proactive_adaptations: u64,
    pub reactive_adaptations: u64,
    pub avg_prediction_error: f64,
}

#[derive(Debug, Clone)]
pub struct WindowMetric {
    pub timestamp: Instant,
    pub batch_size: usize,
    pub processing_time: Duration,
    pub success_rate: f64,
    pub queue_depth: usize,
    pub system_load: f64,
    pub throughput: f64,
}

/// –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–≤–æ–π–Ω–æ–≥–æ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (–•–æ–ª—å—Ç–∞-–í–∏–Ω—Ç–µ—Ä—Å–∞)
#[derive(Debug, Clone)]
struct PredictionModel {
    alpha: f64,          // –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —É—Ä–æ–≤–Ω—è
    beta: f64,           // –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
    gamma: f64,          // –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏
    level: f64,          // –¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å
    trend: f64,          // –¢–µ–∫—É—â–∏–π —Ç—Ä–µ–Ω–¥
    seasonal: Vec<f64>,  // –°–µ–∑–æ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    season_length: usize,
    confidence: f64,     // –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
}

/// –ü—Ä–µ–¥–∏–∫—Ç–æ—Ä —Ä–∞–±–æ—á–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏
#[derive(Debug, Clone)]
struct WorkloadPredictor {
    queue_depth_history: VecDeque<f64>,
    latency_history: VecDeque<f64>,
    throughput_history: VecDeque<f64>,
    pattern_detector: WorkloadPatternDetector,
    prediction_window: usize,
}

#[derive(Debug, Clone)]
struct WorkloadPatternDetector {
    burst_detected: bool,
    steady_state: bool,
    increasing_trend: bool,
    decreasing_trend: bool,
    pattern_confidence: f64,
}

/// –ó–∞–ø–∏—Å—å –∏—Å—Ç–æ—Ä–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
#[derive(Debug, Clone)]
pub struct AdaptationRecord {
    pub timestamp: Instant,
    pub old_size: usize,
    pub new_size: usize,
    pub predicted_improvement: f64,
    pub actual_improvement: f64,
    pub prediction_error: f64,
    pub adaptation_type: AdaptationType,
    pub workload_pattern: WorkloadPattern,
}

/// –¢–∏–ø –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
#[derive(Debug, Clone, PartialEq)]
pub enum AdaptationType {
    Proactive,  // –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    Reactive,   // –†–µ–∞–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    Corrective, // –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
}

/// –ü–∞—Ç—Ç–µ—Ä–Ω —Ä–∞–±–æ—á–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏
#[derive(Debug, Clone)]
pub enum WorkloadPattern {
    Bursty,        // –í—Å–ø–ª–µ—Å–∫ –Ω–∞–≥—Ä—É–∑–∫–∏
    Steady,        // –°—Ç–∞–±–∏–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    Increasing,    // –†–∞—Å—Ç—É—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    Decreasing,    // –ü–∞–¥–∞—é—â–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    Periodic,      // –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
}

/// –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
#[derive(Debug, Clone)]
pub struct LoadPrediction {
    pub predicted_queue: f64,
    pub predicted_latency: f64,
    pub queue_trend: f64,
    pub latency_trend: f64,
    pub confidence: f64,
    pub pattern: WorkloadPattern,
    pub horizon: usize,
}

impl Default for LoadPrediction {
    fn default() -> Self {
        Self {
            predicted_queue: 0.0,
            predicted_latency: 0.0,
            queue_trend: 0.0,
            latency_trend: 0.0,
            confidence: 0.5,
            pattern: WorkloadPattern::Steady,
            horizon: 0,
        }
    }
}

impl PredictionModel {
    fn new(alpha: f64, beta: f64, gamma: f64, season_length: usize) -> Self {
        Self {
            alpha,
            beta,
            gamma,
            level: 0.0,
            trend: 0.0,
            seasonal: vec![0.0; season_length],
            season_length,
            confidence: 0.5,
        }
    }

    /// –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω–æ–≤—ã–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ–º
    fn update(&mut self, observation: f64, timestamp: Instant) -> f64 {
        let season_index = (timestamp.elapsed().as_secs() as usize) % self.season_length;

        // –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        let prediction = self.level + self.trend + self.seasonal[season_index];
        let error = observation - prediction;

        // –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        let last_level = self.level;
        self.level = self.alpha * (observation - self.seasonal[season_index])
            + (1.0 - self.alpha) * (self.level + self.trend);
        self.trend = self.beta * (self.level - last_level)
            + (1.0 - self.beta) * self.trend;
        self.seasonal[season_index] = self.gamma * (observation - last_level - self.trend)
            + (1.0 - self.gamma) * self.seasonal[season_index];

        // –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        let error_abs = error.abs();
        self.confidence = 0.9 * self.confidence + 0.1 * (1.0 - error_abs / (observation + 1.0));

        prediction
    }

    /// –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç
    fn predict(&self, horizon: usize, timestamp: Instant) -> Vec<f64> {
        let mut predictions = Vec::with_capacity(horizon);
        let base_time = timestamp.elapsed().as_secs() as usize;

        for i in 1..=horizon {
            let season_index = (base_time + i) % self.season_length;
            let prediction = self.level + (self.trend * i as f64) + self.seasonal[season_index];
            predictions.push(prediction);
        }

        predictions
    }
}

impl WorkloadPredictor {
    fn new(prediction_window: usize) -> Self {
        Self {
            queue_depth_history: VecDeque::with_capacity(prediction_window * 2),
            latency_history: VecDeque::with_capacity(prediction_window * 2),
            throughput_history: VecDeque::with_capacity(prediction_window * 2),
            pattern_detector: WorkloadPatternDetector::new(),
            prediction_window,
        }
    }

    fn add_observation(&mut self, queue_depth: f64, latency: f64, throughput: f64) {
        self.queue_depth_history.push_back(queue_depth);
        self.latency_history.push_back(latency);
        self.throughput_history.push_back(throughput);

        if self.queue_depth_history.len() > self.prediction_window * 2 {
            self.queue_depth_history.pop_front();
            self.latency_history.pop_front();
            self.throughput_history.pop_front();
        }

        self.pattern_detector.update(
            &self.queue_depth_history,
            &self.latency_history,
        );
    }

    /// –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏
    fn predict_load(&self, horizon: usize) -> LoadPrediction {
        if self.queue_depth_history.len() < 10 {
            return LoadPrediction::default();
        }

        // –ü—Ä–æ—Å—Ç–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        let recent_queue: Vec<f64> = self.queue_depth_history.iter().copied().collect();
        let recent_latency: Vec<f64> = self.latency_history.iter().copied().collect();

        let queue_trend = Self::calculate_trend(&recent_queue);
        let latency_trend = Self::calculate_trend(&recent_latency);

        let current_queue = recent_queue.last().copied().unwrap_or(0.0);
        let current_latency = recent_latency.last().copied().unwrap_or(0.0);

        let predicted_queue = current_queue + queue_trend * horizon as f64;
        let predicted_latency = current_latency + latency_trend * horizon as f64;

        let confidence = self.pattern_detector.pattern_confidence;
        let pattern = self.pattern_detector.detect_pattern();

        LoadPrediction {
            predicted_queue,
            predicted_latency,
            queue_trend,
            latency_trend,
            confidence,
            pattern,
            horizon,
        }
    }

    fn calculate_trend(data: &[f64]) -> f64 {
        if data.len() < 2 {
            return 0.0;
        }

        let n = data.len() as f64;
        let sum_x: f64 = (0..data.len()).map(|i| i as f64).sum();
        let sum_y: f64 = data.iter().sum();
        let sum_xy: f64 = data.iter().enumerate().map(|(i, &y)| i as f64 * y).sum();
        let sum_x2: f64 = (0..data.len()).map(|i| (i as f64).powi(2)).sum();

        let numerator = n * sum_xy - sum_x * sum_y;
        let denominator = n * sum_x2 - sum_x * sum_x;

        if denominator.abs() > 1e-10 {
            numerator / denominator
        } else {
            0.0
        }
    }
}

impl WorkloadPatternDetector {
    fn new() -> Self {
        Self {
            burst_detected: false,
            steady_state: false,
            increasing_trend: false,
            decreasing_trend: false,
            pattern_confidence: 0.5,
        }
    }

    fn update(&mut self, queue_history: &VecDeque<f64>, latency_history: &VecDeque<f64>) {
        if queue_history.len() < 5 {
            return;
        }

        let queue_data: Vec<f64> = queue_history.iter().copied().collect();
        let latency_data: Vec<f64> = latency_history.iter().copied().collect();

        // –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—Å–ø–ª–µ—Å–∫–æ–≤ (burst)
        let queue_mean: f64 = queue_data.iter().sum::<f64>() / queue_data.len() as f64;
        let queue_std = Self::calculate_std(&queue_data, queue_mean);
        let last_queue = *queue_data.last().unwrap();

        self.burst_detected = last_queue > queue_mean + 2.0 * queue_std;

        // –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤
        let queue_trend = WorkloadPredictor::calculate_trend(&queue_data);
        let latency_trend = WorkloadPredictor::calculate_trend(&latency_data);

        self.increasing_trend = queue_trend > 0.1 || latency_trend > 0.05;
        self.decreasing_trend = queue_trend < -0.1 || latency_trend < -0.05;
        self.steady_state = !self.burst_detected && !self.increasing_trend && !self.decreasing_trend;

        // –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        let burst_confidence = if self.burst_detected { 0.8 } else { 0.2 };
        let trend_confidence = queue_trend.abs().min(1.0);
        self.pattern_confidence = 0.7 * burst_confidence + 0.3 * trend_confidence;
    }

    fn calculate_std(data: &[f64], mean: f64) -> f64 {
        let variance: f64 = data.iter()
            .map(|&x| (x - mean).powi(2))
            .sum::<f64>() / data.len() as f64;
        variance.sqrt()
    }

    fn detect_pattern(&self) -> WorkloadPattern {
        if self.burst_detected {
            WorkloadPattern::Bursty
        } else if self.increasing_trend {
            WorkloadPattern::Increasing
        } else if self.decreasing_trend {
            WorkloadPattern::Decreasing
        } else if self.steady_state {
            WorkloadPattern::Steady
        } else {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
            // –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏
            WorkloadPattern::Steady
        }
    }
}

impl AdaptiveBatcher {
    pub fn new(config: AdaptiveBatcherConfig) -> Self {
        info!("üîÑ AdaptiveBatcher –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π");

        let prediction_model = PredictionModel::new(
            config.smoothing_factor,
            config.smoothing_factor * 0.5,
            config.smoothing_factor * 0.3,
            60, // –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å 60 —Å–µ–∫—É–Ω–¥
        );

        let workload_predictor = WorkloadPredictor::new(
            config.prediction_horizon.as_secs() as usize
        );

        Self {
            config: config.clone(),
            current_batch_size: RwLock::new(config.initial_batch_size),
            metrics: RwLock::new(BatchMetrics {
                total_batches: 0,
                total_items: 0,
                avg_batch_size: config.initial_batch_size as f64,
                avg_processing_time: Duration::from_millis(0),
                p95_processing_time: Duration::from_millis(0),
                p99_processing_time: Duration::from_millis(0),
                last_adaptation: Instant::now(),
                adaptation_count: 0,
                prediction_accuracy: 0.5,
                proactive_adaptations: 0,
                reactive_adaptations: 0,
                avg_prediction_error: 0.0,
            }),
            window_metrics: Mutex::new(Vec::new()),
            metrics_store: Arc::new(DashMap::new()),
            prediction_model: Mutex::new(prediction_model),
            workload_predictor: Mutex::new(workload_predictor),
            adaptation_history: Mutex::new(Vec::new()),
        }
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    pub async fn get_batch_size(&self) -> usize {
        *self.current_batch_size.read().await
    }

    /// –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–∞—Ç—á–∞
    pub async fn record_batch_execution(
        &self,
        batch_size: usize,
        processing_time: Duration,
        success_rate: f64,
        queue_depth: usize,
    ) {
        let now = Instant::now();
        let processing_time_ms = processing_time.as_millis() as f64;
        let throughput = batch_size as f64 / (processing_time.as_secs_f64() + 0.001);

        // –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã
        let system_load = (queue_depth as f64 / self.config.max_batch_size as f64)
            .min(1.0);

        // –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–∫–Ω–∞
        {
            let mut window = self.window_metrics.lock().await;
            window.push(WindowMetric {
                timestamp: now,
                batch_size,
                processing_time,
                success_rate,
                queue_depth,
                system_load,
                throughput,
            });

            // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            window.retain(|m| now.duration_since(m.timestamp) <= self.config.window_duration);
        }

        // –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        {
            let mut predictor = self.workload_predictor.lock().await;
            predictor.add_observation(
                queue_depth as f64,
                processing_time_ms,
                throughput,
            );

            let mut model = self.prediction_model.lock().await;
            model.update(processing_time_ms, now);
        }

        // –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        {
            let mut metrics = self.metrics.write().await;
            metrics.total_batches += 1;
            metrics.total_items += batch_size as u64;

            // –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            let alpha = 0.1;
            metrics.avg_batch_size = metrics.avg_batch_size * (1.0 - alpha) + batch_size as f64 * alpha;
            metrics.avg_processing_time = Duration::from_nanos(
                (metrics.avg_processing_time.as_nanos() as f64 * (1.0 - alpha) +
                    processing_time.as_nanos() as f64 * alpha) as u64
            );
        }

        // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.record_metric("batch_size".to_string(), batch_size as f64);
        self.record_metric("processing_time_ms".to_string(), processing_time_ms);
        self.record_metric("success_rate".to_string(), success_rate);
        self.record_metric("queue_depth".to_string(), queue_depth as f64);
        self.record_metric("system_load".to_string(), system_load);
        self.record_metric("throughput".to_string(), throughput);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.check_adaptation().await;
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    async fn check_adaptation(&self) {
        let now = Instant::now();
        let last_adaptation = self.metrics.read().await.last_adaptation;

        if now.duration_since(last_adaptation) < self.config.adaptation_interval {
            return;
        }

        if !self.config.enable_auto_tuning {
            return;
        }

        // –í—ã–ø–æ–ª–Ω—è–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏—é —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        self.perform_predictive_adaptation().await;
    }

    /// –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é
    async fn perform_predictive_adaptation(&self) {
        let window_metrics = self.window_metrics.lock().await.clone();

        if window_metrics.is_empty() {
            return;
        }

        // –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        let current_batch_size = *self.current_batch_size.read().await;
        let target_latency_ns = self.config.target_latency.as_nanos() as f64;

        // –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∑–∞ –æ–∫–Ω–æ
        let avg_processing_time = window_metrics.iter()
            .map(|m| m.processing_time.as_nanos() as f64)
            .sum::<f64>() / window_metrics.len() as f64;

        let avg_success_rate = window_metrics.iter()
            .map(|m| m.success_rate)
            .sum::<f64>() / window_metrics.len() as f64;

        let avg_queue_depth = window_metrics.iter()
            .map(|m| m.queue_depth as f64)
            .sum::<f64>() / window_metrics.len() as f64;

        let avg_system_load = window_metrics.iter()
            .map(|m| m.system_load)
            .sum::<f64>() / window_metrics.len() as f64;

        // –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
        let load_prediction = {
            let predictor = self.workload_predictor.lock().await;
            let horizon = self.config.prediction_horizon.as_secs() as usize / 5; // 5-—Å–µ–∫—É–Ω–¥–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            predictor.predict_load(horizon.min(12)) // –ú–∞–∫—Å–∏–º—É–º 12 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ (1 –º–∏–Ω—É—Ç–∞)
        };

        // –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏
        let latency_prediction = {
            let model = self.prediction_model.lock().await;
            let horizon = self.config.prediction_horizon.as_secs() as usize / 5;
            model.predict(horizon.min(12), Instant::now())
        };

        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        let adaptation_type = if load_prediction.confidence > self.config.confidence_threshold &&
            self.config.enable_predictive_adaptation {
            AdaptationType::Proactive
        } else if avg_processing_time > target_latency_ns * 1.3 {
            AdaptationType::Reactive
        } else {
            AdaptationType::Corrective
        };

        // –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        let new_batch_size = match adaptation_type {
            AdaptationType::Proactive => {
                self.perform_proactive_adaptation(
                    current_batch_size,
                    &load_prediction,
                    &latency_prediction,
                    avg_system_load,
                )
            }
            AdaptationType::Reactive => {
                self.perform_reactive_adaptation(
                    current_batch_size,
                    avg_processing_time,
                    target_latency_ns,
                    avg_success_rate,
                    avg_queue_depth,
                )
            }
            AdaptationType::Corrective => {
                self.perform_corrective_adaptation(
                    current_batch_size,
                    avg_processing_time,
                    target_latency_ns,
                    avg_success_rate,
                    avg_system_load,
                )
            }
        };

        // –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        if new_batch_size != current_batch_size {
            let old_size = current_batch_size;
            *self.current_batch_size.write().await = new_batch_size;

            let mut metrics = self.metrics.write().await;
            metrics.last_adaptation = Instant::now();
            metrics.adaptation_count += 1;

            // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–¥–∞–ø—Ç–∞—Ü–∏–π
            match adaptation_type {
                AdaptationType::Proactive => metrics.proactive_adaptations += 1,
                AdaptationType::Reactive => metrics.reactive_adaptations += 1,
                _ => {}
            }

            // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            let predicted_improvement = self.calculate_predicted_improvement(
                old_size,
                new_batch_size,
                avg_processing_time,
                &load_prediction,
            );

            self.record_adaptation_history(
                old_size,
                new_batch_size,
                predicted_improvement,
                avg_processing_time,
                adaptation_type.clone(),
                load_prediction.pattern.clone(),
            ).await;

            info!("üîÑ AdaptiveBatcher: {} –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Å {} –Ω–∞ {} (–∑–∞–¥–µ—Ä–∂–∫–∞: {:.1}–º—Å, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {:.1}%, –ø–∞—Ç—Ç–µ—Ä–Ω: {:?})",
                  match adaptation_type {
                      AdaptationType::Proactive => "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è",
                      AdaptationType::Reactive => "–†–µ–∞–∫—Ç–∏–≤–Ω–∞—è",
                      AdaptationType::Corrective => "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è",
                  },
                  old_size, new_batch_size,
                  avg_processing_time / 1_000_000.0,
                  load_prediction.confidence * 100.0,
                  load_prediction.pattern);

            self.record_metric("adapted_batch_size".to_string(), new_batch_size as f64);
            self.record_metric("adaptation_count".to_string(), metrics.adaptation_count as f64);
            self.record_metric("adaptation_type".to_string(),
                               match adaptation_type {
                                   AdaptationType::Proactive => 0.0,
                                   AdaptationType::Reactive => 1.0,
                                   AdaptationType::Corrective => 2.0,
                               });
            self.record_metric("prediction_confidence".to_string(), load_prediction.confidence);
        }
    }

    /// –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    fn perform_proactive_adaptation(
        &self,
        current_size: usize,
        load_prediction: &LoadPrediction,
        latency_prediction: &[f64],
        current_load: f64,
    ) -> usize {
        let predicted_latency = load_prediction.predicted_latency;
        let _predicted_queue = load_prediction.predicted_queue; // –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ª–æ–≥–∏–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
        let confidence = load_prediction.confidence;

        // –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        let mut new_size = current_size;

        match &load_prediction.pattern {
            WorkloadPattern::Bursty => {
                // –ü—Ä–∏ –≤—Å–ø–ª–µ—Å–∫–µ –Ω–∞–≥—Ä—É–∑–∫–∏ —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∞–∫—Ü–∏–∏
                let reduction_factor = 1.0 - (0.3 * confidence).min(0.5);
                new_size = (current_size as f64 * reduction_factor).ceil() as usize;
            }
            WorkloadPattern::Increasing => {
                // –ü—Ä–∏ —Ä–∞—Å—Ç—É—â–µ–π –Ω–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä
                let trend_factor = load_prediction.queue_trend.abs().min(0.5);
                let reduction = 1.0 - (0.2 * trend_factor * confidence);
                new_size = (current_size as f64 * reduction).ceil() as usize;
            }
            WorkloadPattern::Decreasing => {
                // –ü—Ä–∏ –ø–∞–¥–∞—é—â–µ–π –Ω–∞–≥—Ä—É–∑–∫–µ –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä
                let trend_factor = load_prediction.queue_trend.abs().min(0.3);
                let increase = 1.0 + (0.25 * trend_factor * confidence);
                new_size = (current_size as f64 * increase).ceil() as usize;
            }
            WorkloadPattern::Steady => {
                // –°—Ç–∞–±–∏–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏
                if predicted_latency > self.config.target_latency.as_millis() as f64 * 1.1 {
                    let reduction = (predicted_latency / self.config.target_latency.as_millis() as f64)
                        .min(2.0);
                    new_size = (current_size as f64 / reduction).ceil() as usize;
                } else if predicted_latency < self.config.target_latency.as_millis() as f64 * 0.7 {
                    let increase = 1.0 + (self.config.max_increase_rate * confidence).min(0.4);
                    new_size = (current_size as f64 * increase).ceil() as usize;
                }
            }
            WorkloadPattern::Periodic => {
                // –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
                if let Some(&next_latency) = latency_prediction.first() {
                    if next_latency > self.config.target_latency.as_millis() as f64 {
                        new_size = (current_size as f64 * 0.8).ceil() as usize;
                    }
                }
            }
        }

        // –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã
        if current_load > 0.8 {
            new_size = (new_size as f64 * 0.9).ceil() as usize;
        }

        new_size.max(self.config.min_batch_size).min(self.config.max_batch_size)
    }

    /// –†–µ–∞–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    fn perform_reactive_adaptation(
        &self,
        current_size: usize,
        avg_processing_time: f64,
        target_latency_ns: f64,
        avg_success_rate: f64,
        avg_queue_depth: f64,
    ) -> usize {
        if avg_processing_time > target_latency_ns * 1.2 {
            // –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ - —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            let decrease_factor = (avg_processing_time / target_latency_ns).min(2.0);
            let new_size = (current_size as f64 / decrease_factor).ceil() as usize;
            new_size.max(self.config.min_batch_size)
        } else if avg_processing_time < target_latency_ns * 0.8 && avg_success_rate > 0.95 {
            // –ó–∞–¥–µ—Ä–∂–∫–∞ –Ω–∏–∑–∫–∞—è, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–∞—è - –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å
            let increase_factor = 1.0 + self.config.max_increase_rate.min(0.5);
            let new_size = (current_size as f64 * increase_factor).ceil() as usize;
            new_size.min(self.config.max_batch_size)
        } else if avg_queue_depth > (current_size * 2) as f64 {
            // –ë–æ–ª—å—à–∞—è –æ—á–µ—Ä–µ–¥—å - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            let increase_factor = 1.0 + (avg_queue_depth / current_size as f64).min(0.3);
            let new_size = (current_size as f64 * increase_factor).ceil() as usize;
            new_size.min(self.config.max_batch_size)
        } else {
            // –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä
            current_size
        }
    }

    /// –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    fn perform_corrective_adaptation(
        &self,
        current_size: usize,
        avg_processing_time: f64,
        target_latency_ns: f64,
        avg_success_rate: f64,
        avg_system_load: f64,
    ) -> usize {
        let latency_ratio = avg_processing_time / target_latency_ns;

        if latency_ratio > 1.1 && avg_success_rate < 0.9 {
            // –ù–µ–±–æ–ª—å—à–æ–µ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏ —Å –Ω–∏–∑–∫–æ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç—å—é
            let reduction = 1.0 - (0.1 * (latency_ratio - 1.0)).min(0.2);
            (current_size as f64 * reduction).ceil() as usize
        } else if latency_ratio < 0.9 && avg_success_rate > 0.98 && avg_system_load < 0.7 {
            // –•–æ—Ä–æ—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            let increase = 1.0 + (0.1 * (1.0 - latency_ratio)).min(0.15);
            (current_size as f64 * increase).ceil() as usize
        } else {
            current_size
        }.max(self.config.min_batch_size).min(self.config.max_batch_size)
    }

    fn calculate_predicted_improvement(
        &self,
        old_size: usize,
        new_size: usize,
        _current_latency: f64,
        load_prediction: &LoadPrediction,
    ) -> f64 {
        if old_size == new_size {
            return 0.0;
        }

        let size_ratio = new_size as f64 / old_size as f64;
        let latency_trend = load_prediction.latency_trend;

        // –û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Ç—Ä–µ–Ω–¥–µ –∑–∞–¥–µ—Ä–∂–∫–∏
        let size_effect = if size_ratio < 1.0 {
            // –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–ª–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É
            1.0 / size_ratio - 1.0
        } else {
            // –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É
            -(size_ratio - 1.0) * 0.5
        };

        let trend_effect = -latency_trend * 10.0; // –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Ç—Ä–µ–Ω–¥–∞

        (size_effect + trend_effect) * load_prediction.confidence
    }

    async fn record_adaptation_history(
        &self,
        old_size: usize,
        new_size: usize,
        predicted_improvement: f64,
        _current_processing_time: f64,
        adaptation_type: AdaptationType,
        workload_pattern: WorkloadPattern,
    ) {
        let mut history = self.adaptation_history.lock().await;

        let record = AdaptationRecord {
            timestamp: Instant::now(),
            old_size,
            new_size,
            predicted_improvement,
            actual_improvement: 0.0, // –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            prediction_error: 0.0,
            adaptation_type,
            workload_pattern,
        };

        // –û–±–Ω–æ–≤–ª—è–µ–º actual_improvement –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–ø–∏—Å–∏, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if let Some(prev_record) = history.last_mut() {
            if prev_record.actual_improvement == 0.0 {
                // –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
                let size_change_percentage = if prev_record.old_size == 0 {
                    0.0
                } else {
                    (new_size as f64 / prev_record.old_size as f64 - 1.0) * 100.0
                };

                // –ü—Ä–æ—Å—Ç–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è: –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ = —É–ª—É—á—à–µ–Ω–∏–µ
                let actual_improvement = -size_change_percentage / 100.0;

                prev_record.actual_improvement = actual_improvement.max(-1.0).min(1.0);
                prev_record.prediction_error = (actual_improvement - prev_record.predicted_improvement).abs();
            }
        }

        history.push(record);

        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if history.len() > 100 {
            history.remove(0);
        }

        // –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
        self.update_prediction_accuracy().await;
    }

    async fn update_prediction_accuracy(&self) {
        let history = self.adaptation_history.lock().await;

        if history.len() < 5 {
            return;
        }

        let mut total_error = 0.0;
        let mut count = 0;

        for record in history.iter() {
            if record.prediction_error > 0.0 {
                total_error += record.prediction_error;
                count += 1;
            }
        }

        if count > 0 {
            let avg_error = total_error / count as f64;
            let accuracy = 1.0 - avg_error.min(1.0);

            let mut metrics = self.metrics.write().await;
            metrics.prediction_accuracy = accuracy;
            metrics.avg_prediction_error = avg_error;

            self.record_metric("prediction_accuracy".to_string(), accuracy);
            self.record_metric("avg_prediction_error".to_string(), avg_error);
        }
    }

    fn record_metric(&self, key: String, value: f64) {
        self.metrics_store.insert(
            format!("adaptive_batcher.{}", key),
            value
        );
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    pub async fn get_metrics(&self) -> BatchMetrics {
        self.metrics.read().await.clone()
    }

    /// –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ–∫–Ω–∞
    pub async fn get_window_metrics(&self) -> Vec<WindowMetric> {
        self.window_metrics.lock().await.clone()
    }

    /// –ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–π
    pub async fn get_adaptation_history(&self, limit: usize) -> Vec<AdaptationRecord> {
        let history = self.adaptation_history.lock().await;
        let start = if history.len() > limit {
            history.len() - limit
        } else {
            0
        };
        history[start..].to_vec()
    }

    /// –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
    pub async fn get_load_prediction(&self, horizon_seconds: u64) -> LoadPrediction {
        let predictor = self.workload_predictor.lock().await;
        let horizon = (horizon_seconds / 5).max(1) as usize; // 5-—Å–µ–∫—É–Ω–¥–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        predictor.predict_load(horizon.min(60)) // –ú–∞–∫—Å–∏–º—É–º 60 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ (5 –º–∏–Ω—É—Ç)
    }

    /// –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏—é
    pub async fn force_adaptation(&self) {
        self.perform_predictive_adaptation().await;
    }

    /// –°–±—Ä–æ—Å–∏—Ç—å –∫ –Ω–∞—á–∞–ª—å–Ω—ã–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º
    pub async fn reset(&self) {
        *self.current_batch_size.write().await = self.config.initial_batch_size;

        let mut metrics = self.metrics.write().await;
        *metrics = BatchMetrics {
            total_batches: 0,
            total_items: 0,
            avg_batch_size: self.config.initial_batch_size as f64,
            avg_processing_time: Duration::from_millis(0),
            p95_processing_time: Duration::from_millis(0),
            p99_processing_time: Duration::from_millis(0),
            last_adaptation: Instant::now(),
            adaptation_count: 0,
            prediction_accuracy: 0.5,
            proactive_adaptations: 0,
            reactive_adaptations: 0,
            avg_prediction_error: 0.0,
        };

        self.window_metrics.lock().await.clear();
        self.metrics_store.clear();

        // –°–±—Ä–æ—Å –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        *self.prediction_model.lock().await = PredictionModel::new(
            self.config.smoothing_factor,
            self.config.smoothing_factor * 0.5,
            self.config.smoothing_factor * 0.3,
            60,
        );

        *self.workload_predictor.lock().await = WorkloadPredictor::new(
            self.config.prediction_horizon.as_secs() as usize
        );

        self.adaptation_history.lock().await.clear();

        info!("üîÑ AdaptiveBatcher —Å–±—Ä–æ—à–µ–Ω –∫ –Ω–∞—á–∞–ª—å–Ω—ã–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º");
    }
}

impl Default for AdaptiveBatcherConfig {
    fn default() -> Self {
        Self {
            min_batch_size: 32,
            max_batch_size: 1024,
            initial_batch_size: 128,
            window_duration: Duration::from_secs(5),
            target_latency: Duration::from_millis(50),
            max_increase_rate: 0.5,
            min_decrease_rate: 0.3,
            adaptation_interval: Duration::from_secs(1),
            enable_auto_tuning: true,
            enable_predictive_adaptation: true,
            prediction_horizon: Duration::from_secs(30), // 30-—Å–µ–∫—É–Ω–¥–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            smoothing_factor: 0.3,                      // –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
            confidence_threshold: 0.7,                   // –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        }
    }
}

// –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –∏–º–ø–æ—Ä—Ç
use std::collections::VecDeque;